{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import tornado.web\n",
    "\n",
    "from tornado.web import HTTPError\n",
    "from tornado.httpserver import HTTPServer\n",
    "from tornado.ioloop import IOLoop\n",
    "from tornado.options import define, options\n",
    "\n",
    "from basehandler import BaseHandler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "import pickle\n",
    "from bson.binary import Binary\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UpdateModelForDatasetId():\n",
    "    def __init__(self):\n",
    "        '''Store necessary handlers,\n",
    "           connect to database\n",
    "        '''\n",
    "\n",
    "        try:\n",
    "            self.client  = MongoClient(serverSelectionTimeoutMS=50) # local host, default port\n",
    "            print(self.client.server_info()) # force pymongo to look for possible running servers, error if none running\n",
    "            # if we get here, at least one instance of pymongo is running\n",
    "            self.db = self.client.sklearndatabase # database with labeledinstances, models\n",
    "\n",
    "        except ServerSelectionTimeoutError as inst:\n",
    "            print('Could not initialize database connection, stopping execution')\n",
    "            print('Are you running a valid local-hosted instance of mongodb?')\n",
    "            #raise inst\n",
    "\n",
    "        self.clf = {} # the classifier model (in-class assignment, you might need to change this line!)\n",
    "        # but depending on your implementation, you may not need to change it  \n",
    "\n",
    "        settings = {'debug':True}\n",
    "\n",
    "    def __exit__(self):\n",
    "        self.client.close() # just in case\n",
    "\n",
    "    def update_model(self,data):\n",
    "        '''Train a new model (or update) for given dataset ID\n",
    "        '''\n",
    "        dsid = data['dsid']\n",
    "\n",
    "\n",
    "        # create feature vectors from database\n",
    "        f=[];\n",
    "        for a in self.db.labeledinstances.find({\"dsid\":dsid}):\n",
    "            f.append([float(val) for val in a['feature']])\n",
    "\n",
    "        # create label vector from database\n",
    "        l=[];\n",
    "        for a in self.db.labeledinstances.find({\"dsid\":dsid}):\n",
    "            l.append(a['label'])\n",
    "\n",
    "        # fit the model to the data\n",
    "        c1 = KNeighborsClassifier(n_neighbors=10)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # c1 = RandomForestClassifier(n_estimators=25);\n",
    "        acc = -1;\n",
    "        if l:\n",
    "            \n",
    "            \n",
    "            #testing different numbers of hidden layers. 100 is default\n",
    "#             param_range = [1, 5,10, 50,100]\n",
    "\n",
    "#             train_scores, test_scores = validation_curve(\n",
    "#                             estimator=c1, \n",
    "#                             X=X, \n",
    "#                             y=y, \n",
    "#                             param_name='clf__hidden_layer_sizes', \n",
    "#                             param_range=param_range,\n",
    "#                             scoring=scorer,\n",
    "#                             cv=cv,\n",
    "#                             n_jobs=-1)\n",
    "            \n",
    "#             title = \"Learning Curves (Naive Bayes)\"\n",
    "#             # Cross validation with 100 iterations to get smoother mean test and train\n",
    "#             # score curves, each time with 20% data randomly selected as a validation set.\n",
    "#             cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
    "\n",
    "#             estimator = GaussianNB()\n",
    "#             plot_learning_curve(estimator, title, f, l, ylim=(0.2, 1.01), cv=cv, n_jobs=4)\n",
    "            \n",
    "            \n",
    "#             title = \"Learning Curves (Logistic Regression)\"\n",
    "#             # SVC is more expensive so we do a lower number of CV iterations:\n",
    "#             cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "#             estimator = LogisticRegression()\n",
    "#             plot_learning_curve(estimator, title, f, l, (0.2, 1.01), cv=cv, n_jobs=4)\n",
    "            \n",
    "            title = \"Learning Curves (SVM, RBF kernel, $\\gamma=0.1$)\"\n",
    "            # SVC is more expensive so we do a lower number of CV iterations:\n",
    "            cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "            estimator = SVC(gamma=0.1)\n",
    "            plot_learning_curve(estimator, title, f, l, (0.2, 1.01), cv=cv, n_jobs=4)\n",
    "            \n",
    "\n",
    "            title = \"Learning Curves (SVM, RBF kernel, $\\gamma=0.5$)\"\n",
    "            # SVC is more expensive so we do a lower number of CV iterations:\n",
    "            cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "            estimator = SVC(gamma=0.5)\n",
    "            plot_learning_curve(estimator, title, f, l, (0.2, 1.01), cv=cv, n_jobs=4)\n",
    "            \n",
    "            title = \"Learning Curves (SVM, RBF kernel, $\\gamma=1.0$)\"\n",
    "            # SVC is more expensive so we do a lower number of CV iterations:\n",
    "            cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "            estimator = SVC(gamma=1.0)\n",
    "            plot_learning_curve(estimator, title, f, l, (0.2, 1.01), cv=cv, n_jobs=4)\n",
    "            \n",
    "            title = \"Learning Curves (SVM, RBF kernel, $\\gamma=5.0$)\"\n",
    "            # SVC is more expensive so we do a lower number of CV iterations:\n",
    "            cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "            estimator = SVC(gamma=5.0)\n",
    "            plot_learning_curve(estimator, title, f, l, (0.2, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "            \n",
    "#             title = \"Learning Curves (KNN, k=3)\"\n",
    "#             # SVC is more expensive so we do a lower number of CV iterations:\n",
    "#             cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "#             estimator = KNeighborsClassifier(n_neighbors=3);\n",
    "#             plot_learning_curve(estimator, title, f, l, (0.2, 1.01), cv=cv, n_jobs=4)\n",
    "            \n",
    "#             title = \"Learning Curves (KNN, k=10)\"\n",
    "#             # SVC is more expensive so we do a lower number of CV iterations:\n",
    "#             cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "#             estimator = KNeighborsClassifier(n_neighbors=10);\n",
    "#             plot_learning_curve(estimator, title, f, l, (0.2, 1.01), cv=cv, n_jobs=4)\n",
    "            \n",
    "#             title = \"Learning Curves (KNN, k=20)\"\n",
    "#             # SVC is more expensive so we do a lower number of CV iterations:\n",
    "#             cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "#             estimator = KNeighborsClassifier(n_neighbors=20);\n",
    "#             plot_learning_curve(estimator, title, f, l, (0.2, 1.01), cv=cv, n_jobs=4)\n",
    "            \n",
    "            \n",
    "#             title = \"Learning Curves (Random Forest, n_estimators=10)\"\n",
    "#             # SVC is more expensive so we do a lower number of CV iterations:\n",
    "#             cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "#             estimator = RandomForestClassifier(n_estimators=10);\n",
    "#             plot_learning_curve(estimator, title, f, l, (0.2, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "            \n",
    "#             title = \"Learning Curves (Random Forest, n_estimators=25)\"\n",
    "#             # SVC is more expensive so we do a lower number of CV iterations:\n",
    "#             cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "#             estimator = RandomForestClassifier(n_estimators=25);\n",
    "#             plot_learning_curve(estimator, title, f, l, (0.2, 1.01), cv=cv, n_jobs=4)\n",
    "            \n",
    "#             title = \"Learning Curves (Random Forest, n_estimators=50)\"\n",
    "#             # SVC is more expensive so we do a lower number of CV iterations:\n",
    "#             cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "#             estimator = RandomForestClassifier(n_estimators=50);\n",
    "#             plot_learning_curve(estimator, title, f, l, (0.2, 1.01), cv=cv, n_jobs=-1)\n",
    "            \n",
    "#             title = \"Learning Curves (MLP, hidden_layer_sizes=5)\"\n",
    "#             # SVC is more expensive so we do a lower number of CV iterations:\n",
    "#             cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "#             estimator = MLPClassifier(max_iter=2000, hidden_layer_sizes=5)\n",
    "#             plot_learning_curve(estimator, title, f, l, (0.2, 1.01), cv=cv, n_jobs=-1)\n",
    "            \n",
    "#             title = \"Learning Curves (MLP, hidden_layer_sizes=10)\"\n",
    "#             # SVC is more expensive so we do a lower number of CV iterations:\n",
    "#             cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "#             estimator = MLPClassifier(max_iter=2000, hidden_layer_sizes=10)\n",
    "#             plot_learning_curve(estimator, title, f, l, (0.2, 1.01), cv=cv, n_jobs=-1)\n",
    "            \n",
    "#             title = \"Learning Curves (MLP, hidden_layer_sizes=20)\"\n",
    "#             # SVC is more expensive so we do a lower number of CV iterations:\n",
    "#             cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "#             estimator = MLPClassifier(max_iter=2000, hidden_layer_sizes=20)\n",
    "#             plot_learning_curve(estimator, title, f, l, (0.2, 1.01), cv=cv, n_jobs=-1)\n",
    "            \n",
    "#             title = \"Learning Curves (MLP, hidden_layer_sizes=50)\"\n",
    "#             # SVC is more expensive so we do a lower number of CV iterations:\n",
    "#             cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "#             estimator = MLPClassifier(max_iter=2000, hidden_layer_sizes=50)\n",
    "#             plot_learning_curve(estimator, title, f, l, (0.2, 1.01), cv=cv, n_jobs=-1)\n",
    "            \n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            c1.fit(f,l) # training\n",
    "            lstar = c1.predict(f)\n",
    "            self.clf[dsid] = c1\n",
    "            acc = sum(lstar==l)/float(len(l))\n",
    "            bytes = pickle.dumps(c1)\n",
    "            self.db.models.update({\"dsid\":dsid},\n",
    "                {  \"$set\": {\"model\":Binary(bytes)}  },\n",
    "                upsert=True)\n",
    "        \n",
    "        print(\"YAY!!\")\n",
    "        # send back the resubstitution accuracy\n",
    "        # if training takes a while, we are blocking tornado!! No!!\n",
    "#         self.write_json({\"resubAccuracy\":acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'buildEnvironment': {'cxx': '/usr/bin/clang++: Apple LLVM version 8.0.0 (clang-800.0.38)', 'cc': '/usr/bin/clang: Apple LLVM version 8.0.0 (clang-800.0.38)', 'ccflags': '-I/usr/local/opt/openssl/include -fno-omit-frame-pointer -fPIC -fno-strict-aliasing -ggdb -pthread -Wall -Wsign-compare -Wno-unknown-pragmas -Winvalid-pch -O2 -Wno-unused-local-typedefs -Wno-unused-function -Wno-unused-private-field -Wno-deprecated-declarations -Wno-tautological-constant-out-of-range-compare -Wno-unused-const-variable -Wno-missing-braces -Wno-inconsistent-missing-override -Wno-potentially-evaluated-expression -Wno-null-conversion -mmacosx-version-min=10.12 -fno-builtin-memcmp', 'target_os': 'osx', 'distmod': '', 'distarch': 'x86_64', 'linkflags': '-L/usr/local/opt/openssl/lib -fPIC -pthread -Wl,-bind_at_load -mmacosx-version-min=10.12', 'target_arch': 'x86_64', 'cxxflags': '-Wnon-virtual-dtor -Woverloaded-virtual -Wpessimizing-move -Wredundant-move -std=c++11'}, 'storageEngines': ['devnull', 'ephemeralForTest', 'mmapv1', 'wiredTiger'], 'debug': False, 'versionArray': [3, 2, 10, 0], 'allocator': 'system', 'modules': [], 'bits': 64, 'javascriptEngine': 'mozjs', 'maxBsonObjectSize': 16777216, 'openssl': {'running': 'OpenSSL 1.0.2j  26 Sep 2016', 'compiled': 'OpenSSL 1.0.2j  26 Sep 2016'}, 'sysInfo': 'deprecated', 'gitVersion': '79d9b3ab5ce20f51c272b4411202710a082d0317', 'version': '3.2.10', 'ok': 1.0}\n"
     ]
    }
   ],
   "source": [
    "updater = UpdateModelForDatasetId()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAY!!\n"
     ]
    }
   ],
   "source": [
    "updater.update_model({\"dsid\":208})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [mslc]",
   "language": "python",
   "name": "Python [mslc]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
